{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install Selenium\n","!pip install selenium\n","# Install the webdriver_manager\n","!pip install webdriver_manager\n","# Update and install ChromeDriver\n","!apt-get update\n","!apt install chromium-chromedriver\n","\n","# Copy chromedriver to a known path\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","\n","# Set Chrome options for Selenium\n","from selenium import webdriver\n","\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","\n","# Initialize WebDriver with the specified options\n","driver = webdriver.Chrome(options=chrome_options)\n","\n","\n"],"metadata":{"id":"Gb4mycb86eW5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710883903525,"user_tz":240,"elapsed":65878,"user":{"displayName":"Ryan Handley","userId":"03853269162501385141"}},"outputId":"84eaa7cd-c7cf-45f4-ede1-915e12eb46f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting selenium\n","  Downloading selenium-4.18.1-py3-none-any.whl (10.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n","Collecting trio~=0.17 (from selenium)\n","  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n","  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n","Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.10.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n","Collecting outcome (from trio~=0.17->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n","Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.18.1 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n","Collecting webdriver_manager\n","  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.31.0)\n","Collecting python-dotenv (from webdriver_manager)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.2.2)\n","Installing collected packages: python-dotenv, webdriver_manager\n","Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.1\n","Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [51.0 kB]\n","Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [770 kB]\n","Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,613 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n","Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,080 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,027 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,354 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,074 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,898 kB]\n","Fetched 11.2 MB in 2s (4,960 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n","  udev\n","Suggested packages:\n","  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n","The following NEW packages will be installed:\n","  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n","  systemd-hwe-hwdb udev\n","The following packages will be upgraded:\n","  libudev1\n","1 upgraded, 9 newly installed, 0 to remove and 46 not upgraded.\n","Need to get 26.4 MB of archives.\n","After this operation, 116 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.58+22.04.1 [23.8 MB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n","Fetched 26.4 MB in 2s (14.5 MB/s)\n","Preconfiguring packages ...\n","Selecting previously unselected package apparmor.\n","(Reading database ... 121752 files and directories currently installed.)\n","Preparing to unpack .../apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n","Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n","Selecting previously unselected package liblzo2-2:amd64.\n","Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n","Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n","Selecting previously unselected package squashfs-tools.\n","Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n","Unpacking squashfs-tools (1:4.5-3build1) ...\n","Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n","Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n","Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n","Selecting previously unselected package udev.\n","(Reading database ... 121960 files and directories currently installed.)\n","Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n","Unpacking udev (249.11-0ubuntu3.12) ...\n","Selecting previously unselected package libfuse3-3:amd64.\n","Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n","Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n","Selecting previously unselected package snapd.\n","Preparing to unpack .../snapd_2.58+22.04.1_amd64.deb ...\n","Unpacking snapd (2.58+22.04.1) ...\n","Setting up apparmor (3.0.4-2ubuntu2.3) ...\n","Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n","Setting up liblzo2-2:amd64 (2.10-2build3) ...\n","Setting up squashfs-tools (1:4.5-3build1) ...\n","Setting up udev (249.11-0ubuntu3.12) ...\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n","Setting up snapd (2.58+22.04.1) ...\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.aa-prompt-listener.service → /lib/systemd/system/snapd.aa-prompt-listener.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n","Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n","Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n","Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n","Selecting previously unselected package chromium-browser.\n","(Reading database ... 122193 files and directories currently installed.)\n","Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","=> Installing the chromium snap\n","==> Checking connectivity with the snap store\n","===> System doesn't have a working snapd, skipping\n","Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package systemd-hwe-hwdb.\n","Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n","Unpacking systemd-hwe-hwdb (249.11.5) ...\n","Setting up systemd-hwe-hwdb (249.11.5) ...\n","Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Processing triggers for udev (249.11-0ubuntu3.12) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n","cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"R5iRWlKxuwUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ltl6hI9sJNG_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710883913529,"user_tz":240,"elapsed":629,"user":{"displayName":"Ryan Handley","userId":"03853269162501385141"}},"outputId":"539cfc70-e1e4-4dd3-e7ed-23efa82cbc2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["200\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin, urlparse\n","from requests.adapters import HTTPAdapter\n","from urllib3.util import Retry\n","from selenium import webdriver\n","from selenium.webdriver.chrome.service import Service\n","from webdriver_manager.chrome import ChromeDriverManager\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","import csv\n","\n","# Create a session\n","session = requests.Session()\n","\n","# Define the number of connections you want\n","max_connections = 10\n","\n","# Create an HTTPAdapter with your desired connection pool size\n","adapter = HTTPAdapter(pool_connections=max_connections, pool_maxsize=max_connections)\n","\n","# Mount the adapter to your session for both HTTP and HTTPS\n","session.mount('http://', adapter)\n","session.mount('https://', adapter)\n","\n","response = session.get('https://example.com')\n","print(response.status_code)\n","\n","class AIWebCrawler:\n","    def __init__(self, start_url, max_depth, visit_strategy='preorder'):\n","        self.start_url = start_url\n","        self.max_depth = max_depth\n","        self.visit_strategy = visit_strategy.lower()\n","        self.visited_urls = set()\n","        self.corpus = []  # List to store text content\n","\n","    def get_internal_links(self, page_url, content):\n","        soup = BeautifulSoup(content, 'html.parser')\n","        internal_links = set()\n","\n","        for a_tag in soup.find_all('a', href=True):\n","            href = a_tag['href']\n","            absolute_url = urljoin(page_url, href)\n","\n","            # Check if the link is within the same domain\n","            if urlparse(absolute_url).netloc == urlparse(page_url).netloc:\n","                internal_links.add(absolute_url)\n","\n","        return internal_links\n","\n","    def extract_text_content(self, content):\n","        soup = BeautifulSoup(content, 'html.parser')\n","        # Remove script and style tags\n","        for script_or_style in soup(['script', 'style']):\n","            script_or_style.decompose()\n","        # Extract text content\n","        text_content = soup.get_text(separator=' ', strip=True)\n","        return text_content\n","\n","    def crawl_page(self, driver, url, depth):\n","      if depth == 0 or url in self.visited_urls:\n","        return\n","\n","      try:\n","        # Navigate to the page using Selenium\n","        driver.get(url)\n","        # Optionally wait for a specific element to ensure the page has loaded\n","        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n","\n","        # Now the page's dynamic content should be loaded, extract the page source\n","        content = driver.page_source\n","        # Process the page source with BeautifulSoup as before to extract text and links\n","        text_content = self.extract_text_content(content)\n","        self.corpus.append({'url': url, 'content': text_content})\n","        internal_links = self.get_internal_links(url, content)\n","        self.visited_urls.add(url)\n","\n","        # Recursively crawl the internal links\n","        for link in self.get_ordered_links(internal_links):\n","            self.crawl_page(driver, link, depth - 1)\n","\n","      except Exception as e:\n","        print(f\"Error crawling {url}: {str(e)}\")\n","\n","    def get_ordered_links(self, internal_links):\n","        if self.visit_strategy == 'inorder':\n","            return [self.start_url] + list(internal_links)\n","        elif self.visit_strategy == 'postorder':\n","            return list(internal_links) + [self.start_url]\n","        else:  # Default to preorder\n","            return list(internal_links)\n","\n","    def save_corpus_to_csv(self, csv_filename='corpus.csv'):\n","        with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n","            fieldnames = ['url', 'content']\n","            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","            writer.writeheader()\n","            writer.writerows(self.corpus)\n","\n","    def start_crawling(self):\n","    # Use the driver initialized at the beginning of the script\n","      self.crawl_page(driver, self.start_url, self.max_depth)\n","      self.save_corpus_to_csv()"]},{"cell_type":"markdown","source":["#Evaluation of Other Visiting Strategies#"],"metadata":{"id":"aOl_JUd57Zuu"}},{"cell_type":"markdown","source":["##Preorder Traversal##"],"metadata":{"id":"C_Y7Q-TL9KMI"}},{"cell_type":"code","source":["%%time\n","if __name__ == \"__main__\":\n","    start_url = \"https://www.stjohns.edu/\"\n","    max_depth = 3  # Set the maximum depth of crawling\n","\n","    # Instantiate the WebCrawler with the desired visiting strategy\n","    crawler = AIWebCrawler(start_url, max_depth, visit_strategy='preorder')\n","\n","    # Start crawling and save the corpus to a CSV file\n","    crawler.start_crawling()"],"metadata":{"id":"eGtlm3Xk7e7i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##In-Order Traversal##"],"metadata":{"id":"64WMWG-09PaQ"}},{"cell_type":"code","source":["%%time\n","if __name__ == \"__main__\":\n","    start_url = \"https://www.stjohns.edu/\"\n","    max_depth = 3  # Set the maximum depth of crawling\n","\n","    # Instantiate the WebCrawler with the desired visiting strategy\n","    crawler = AIWebCrawler(start_url, max_depth, visit_strategy='inorder')\n","\n","    # Start crawling and save the corpus to a CSV file\n","    crawler.start_crawling()"],"metadata":{"id":"CbkS65av9YFv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710885315601,"user_tz":240,"elapsed":1383158,"user":{"displayName":"Ryan Handley","userId":"03853269162501385141"}},"outputId":"68b2ce69-068c-4877-c4f8-d0471a851395"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error crawling https://www.stjohns.edu/queens-residential-campus/queens-campus-life/campus-activities: Message: timeout: Timed out receiving message from renderer: 298.486\n","  (Session info: chrome-headless-shell=123.0.6312.58)\n","Stacktrace:\n","#0 0x598ea42c2993 <unknown>\n","#1 0x598ea3fbd136 <unknown>\n","#2 0x598ea3fa5020 <unknown>\n","#3 0x598ea3fa4d43 <unknown>\n","#4 0x598ea3fa2d96 <unknown>\n","#5 0x598ea3fa341f <unknown>\n","#6 0x598ea3fb36e5 <unknown>\n","#7 0x598ea3fc8ebc <unknown>\n","#8 0x598ea3fce48b <unknown>\n","#9 0x598ea3fa3aa2 <unknown>\n","#10 0x598ea3fc8c34 <unknown>\n","#11 0x598ea4048671 <unknown>\n","#12 0x598ea4029a73 <unknown>\n","#13 0x598ea3ffac93 <unknown>\n","#14 0x598ea3ffb65e <unknown>\n","#15 0x598ea428708b <unknown>\n","#16 0x598ea428b005 <unknown>\n","#17 0x598ea4275491 <unknown>\n","#18 0x598ea428bb92 <unknown>\n","#19 0x598ea425a9ef <unknown>\n","#20 0x598ea42b1df8 <unknown>\n","#21 0x598ea42b1fcb <unknown>\n","#22 0x598ea42c1ae4 <unknown>\n","#23 0x79b2f9abeac3 <unknown>\n","\n","Error crawling https://www.stjohns.edu/admission/connect-us/undergraduate-programs-ask-student-or-staff: Message: timeout: Timed out receiving message from renderer: 299.938\n","  (Session info: chrome-headless-shell=123.0.6312.58)\n","Stacktrace:\n","#0 0x598ea42c2993 <unknown>\n","#1 0x598ea3fbd136 <unknown>\n","#2 0x598ea3fa5020 <unknown>\n","#3 0x598ea3fa4d43 <unknown>\n","#4 0x598ea3fa2d96 <unknown>\n","#5 0x598ea3fa341f <unknown>\n","#6 0x598ea3fb36e5 <unknown>\n","#7 0x598ea3fc8ebc <unknown>\n","#8 0x598ea3fce48b <unknown>\n","#9 0x598ea3fa3aa2 <unknown>\n","#10 0x598ea3fc8c34 <unknown>\n","#11 0x598ea4048671 <unknown>\n","#12 0x598ea4029a73 <unknown>\n","#13 0x598ea3ffac93 <unknown>\n","#14 0x598ea3ffb65e <unknown>\n","#15 0x598ea428708b <unknown>\n","#16 0x598ea428b005 <unknown>\n","#17 0x598ea4275491 <unknown>\n","#18 0x598ea428bb92 <unknown>\n","#19 0x598ea425a9ef <unknown>\n","#20 0x598ea42b1df8 <unknown>\n","#21 0x598ea42b1fcb <unknown>\n","#22 0x598ea42c1ae4 <unknown>\n","#23 0x79b2f9abeac3 <unknown>\n","\n","CPU times: user 1min 7s, sys: 1.58 s, total: 1min 8s\n","Wall time: 23min 2s\n"]}]},{"cell_type":"markdown","source":["##Post-Order Traversal##"],"metadata":{"id":"opxgRN059TvE"}},{"cell_type":"code","source":["%%time\n","if __name__ == \"__main__\":\n","    start_url = \"https://www.stjohns.edu/\"\n","    max_depth = 3  # Set the maximum depth of crawling\n","\n","    # Instantiate the WebCrawler with the desired visiting strategy\n","    crawler = AIWebCrawler(start_url, max_depth, visit_strategy='postorder')\n","\n","    # Start crawling and save the corpus to a CSV file\n","    crawler.start_crawling()"],"metadata":{"id":"_jvadno69YnD"},"execution_count":null,"outputs":[]}]}